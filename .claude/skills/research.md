---
name: research
description: Use when user provides a lead (URL, image, or topic) to research comprehensively across GitHub, Reddit, Twitter, LinkedIn, and web sources
---

# Research Agent

Comprehensive multi-source research with parallel subagents.

## Workflow

### 1. Parse Input
- **URL**: Extract topic from page title, path, or domain
- **Image path**: Use Read tool to view image, analyze content, extract subject
- **Text**: Use directly as topic

### 2. Generate Slug
```
lowercase ‚Üí replace [^a-z0-9] with "-" ‚Üí collapse "--" ‚Üí trim ‚Üí max 50 chars
```

Examples:
- "Cursor IDE" ‚Üí "cursor-ide"
- "https://linear.app" ‚Üí "linear"

### 3. Check Existing Entry
```
Glob research/catalogue/*.md
For each: read frontmatter topic:, slugify, compare
Match if slugs equal OR one contains the other
If match: set existing_context with date and "focus on NEW info"
```

### 4. Generate Session ID
Format: `YYYYMMDD-HHMMSS-<slug>`
Create: `.claude/research-cache/<session_id>/`

### 5. Write Plan
Write to `.claude/research-cache/<session_id>/plan.md`:
```markdown
## Research Plan: <topic>
- Input type: [URL/Image/Topic]
- Existing entry: [Yes - path / No]
- Subagents: [github, reddit, twitter, linkedin, web]
```

### 6. Progress Output
Display: `üîç Researching **<topic>** across 5 sources...`

### 7. Dispatch Subagents
For each source [github, reddit, twitter, linkedin, web]:
1. Read `.claude/prompts/<source>-subagent.md`
2. Replace `{{topic}}`, `{{session_id}}`, `{{existing_context}}`
3. Dispatch: `Task(prompt=interpolated, model="opus")`

Dispatch all 5 in parallel (single message, 5 Task calls).

### 8. Check Success Threshold
- Count subagents with non-empty findings
- If < 3 succeeded:
  ```
  ‚ö†Ô∏è Only N/5 sources returned data.
  Failed: [list]
  Continue with limited data? (yes/no)
  ```
- If >= 3: proceed

### 9. Evaluate Completeness
Check 8 dimensions have data:
1. Overview 2. Technical 3. Trends 4. Key Innovation
5. Competitive Landscape 6. Usefulness 7. Pros/Cons 8. Sentiment

If critical gaps AND retries < 2: dispatch targeted follow-up subagent.

### 10. Synthesize
Combine all findings into unified report.
If updating existing: apply merge strategy (replace sentiment/trends, merge others).

### 11. Citation Verification
```
Read .claude/prompts/citation-agent.md
Replace {{topic}}, {{session_id}}
Task(prompt=interpolated, model="sonnet")
```

### 12. Generate Outputs

**Key Insights** (display immediately):
```markdown
## Key Insights: <topic>

**What it is:** [one sentence]
**What's novel:** [key innovation]
**How it compares:** [competitive position]
**Bottom line:** [assessment]

üìÑ Full report: research/catalogue/YYYY-MM-DD-<slug>.md
```

**Full Report**:
- If new: Write to `research/catalogue/YYYY-MM-DD-<slug>.md`
- If update: Edit existing file, update `updated:` frontmatter

**Index Update**:
- If new: Append row to `research/catalogue.md`
- If update: Edit existing row, update date

## Report Template

```markdown
---
topic: <topic>
slug: <slug>
date: YYYY-MM-DD
updated: YYYY-MM-DD
sources: [github, reddit, twitter, linkedin, web]
---

# <Topic> Research Report

## Overview
[2-3 paragraphs]

## Technical Analysis
[From GitHub + Web]

## Key Innovation
[What's different]

## Trends
[Adoption trajectory]

## Competitive Landscape
| Competitor | Differentiation |
|------------|-----------------|

## Usefulness Assessment
**Best for:** [list]
**Not ideal for:** [list]

## Pros and Cons
| Pros | Cons |
|------|------|

## User Sentiment
**Overall:** [Positive/Mixed/Negative]
**Reddit:** [summary]
**Twitter:** [summary]
**LinkedIn:** [summary]

## Sources
[Full list with URLs]

---
*Generated by Research Agent on YYYY-MM-DD*
```

## Dimensions
1. Overview
2. Technical
3. Trends
4. Key Innovation
5. Competitive Landscape
6. Usefulness
7. Pros and Cons
8. User Sentiment (recency-weighted)
