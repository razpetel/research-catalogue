---
topic: ivrit-ai
slug: ivrit-ai
date: 2026-02-19
sources: [github, reddit, twitter, linkedin, web]
---

# ivrit-ai Research Report

## Overview

ivrit-ai is an Israeli non-profit founded in ~2023 by Yair Lifshitz (Technion CS, software CEO), Yanir Marmor (Weizmann Institute speech recognition researcher), and Kinneret Misgav (PhD), with contributor Yoad Snapir (ex-TestFairy/Sauce Labs). The organization's mission is making Hebrew a first-class language in AI by providing high-quality, permissively licensed speech datasets and fine-tuned models at no cost.

The project has produced the world's largest Hebrew audio corpus -- over 22,000 hours as of July 2025 -- collected from Israeli Knesset (parliament) recordings, podcasts, YouTube, crowd-sourced volunteer readings, and WhatsApp voice messages. Their custom "ivrit.ai v1 license" explicitly permits commercial AI training while preserving content creator rights. All models are published on HuggingFace under Apache-2.0, and all code is MIT-licensed on GitHub.

Their flagship models are `whisper-large-v3` (2B params) and `whisper-large-v3-turbo` (0.8B params), both fine-tuned from OpenAI Whisper using a two-phase training pipeline on 8x NVIDIA A40 GPUs. The Interspeech 2025 paper demonstrates up to 29% WER reduction over baseline Whisper. ivrit-ai also maintains the Hebrew Transcription Leaderboard on HuggingFace and has recently expanded to Yiddish with `yi-whisper` model variants. The team operates entirely as volunteers with no compensation -- donations fund only compute resources.

## Technical Analysis

### GitHub Presence (10 repositories, ~127 combined stars)

The organization maintains a comprehensive ecosystem spanning the full pipeline from data collection to deployment:

| Repository | Stars | Purpose | Status |
|-----------|-------|---------|--------|
| ivrit.ai | 45 | Core dataset collection & processing toolkit | Maintained |
| runpod-serverless | 32 | RunPod inference endpoint | Maintained |
| eliezer | 18 | WhatsApp transcription bot | Maintained |
| asr-training | 12 | Training recipes, evaluation suite, model converter | Active |
| ivrit-py | 12 | PyPI package (`pip install ivrit`) | Active |
| transcribe-service | 2 | Web transcription service | Active (latest: 2026-02-18) |
| crowd-transcribe | 3 | Volunteer transcription web app | Low activity |
| explore | 2 | Data exploration tools | Active |
| crowd-recital | 1 | Volunteer recording web app | Low activity |
| hebrew-academy | 0 | Hebrew Academy data tools | Inactive |

Primary maintainers: yairl (Yair Lifshitz) and yoadsn (Yoad Snapir), with external contributors (thewh1teagle, bladefistx2, r-bit-rry) indicating healthy community engagement. A bounty program (200 NIS per issue) incentivizes contributions.

### HuggingFace Ecosystem (21 models, 24 datasets)

**Key Models:**

| Model | Params | Monthly Downloads | License |
|-------|--------|------------------|---------|
| whisper-large-v3 | 2B | 2,762 | Apache-2.0 |
| whisper-large-v3-turbo | 0.8B | 2,142 | Apache-2.0 |
| whisper-large-v3-ct2 | -- | 5,670 | Apache-2.0 |
| whisper-large-v3-turbo-ct2 | -- | 18,400 | Apache-2.0 |
| pyannote-speaker-diarization-3.1 | -- | 18,800 | -- |
| yi-whisper-large-v3 (Yiddish) | 2B | 227 | Apache-2.0 |
| yi-whisper-large-v3-turbo (Yiddish) | 0.8B | 29 | Apache-2.0 |

The CTranslate2 variants are the most downloaded, indicating production deployment preference for optimized inference.

**Training Architecture:**
- Phase 1: Pre-training on ~4,700h of Knesset data (3 epochs, ~54h)
- Phase 2: Mixed post-training on crowd-transcribe-v5 (300h) + crowd-recital (50h) + filtered Knesset (150h), sampling probabilities (0.9, 0.025, 0.075)
- Final model: Weighted average of lowest eval-loss checkpoints
- Hardware: 8x NVIDIA A40, bf16 mixed precision, LR 1e-5, batch 32, ~60h total

**Known Limitations:**
- Language detection degraded (must explicitly set `language="he"`)
- Translation task disabled
- Earlier Turbo fine-tune had "catastrophic forgetting" of long-form transcription (>30s audio); addressed in latest v3 models
- Hallucination issues noted in v2 models

### Academic Publications

1. **"ivrit.ai: A Comprehensive Dataset of Hebrew Speech"** (arXiv 2307.08720, 2023) -- foundational dataset paper. Submitted to ICLR 2024 (rejected). CC-BY 4.0.
2. **"Building an Accurate Open-Source Hebrew ASR System through Crowdsourcing"** (Interspeech 2025, pp. 723-727) -- systems paper demonstrating 29% WER reduction. Accepted at a top speech conference.

## User Sentiment

**Overall: Positive**

- **Reddit:** Thin but meaningful presence. ivrit-ai's datasets have become the de facto benchmark for Hebrew ASR, used even by competitors (SpeechText.AI). Independent benchmarker Daniel Dorman praised the team: "doing important & challenging work with very limited resources." Some users report accuracy issues (3-4 errors per word in certain contexts). The project is seen as "good enough for free" -- better than vanilla Whisper but below commercial alternatives in raw accuracy.

- **Twitter/X:** Zero presence. No official account, no founder accounts, no third-party mentions discovered. This is a deliberate choice -- the project communicates primarily in Hebrew through Israeli platforms (LinkedIn, Facebook groups).

- **LinkedIn:** 5K followers on the company page. Yair Lifshitz and Yanir Marmor post regular updates (mostly in Hebrew). The ElevenLabs scribe_v1 evaluation post (English) showed the team's benchmarking credibility. Ran Balicer (prominent Israeli health-tech figure) shared the leaderboard. Comments raise sustainability concerns about the volunteer model.

## Competitive Landscape

| Competitor | Type | Hebrew Quality | Cost |
|-----------|------|---------------|------|
| **ivrit-ai** | Open-source, non-profit | Best open-source (29% better than Whisper) | Free |
| OpenAI Whisper (vanilla) | Open-source | Baseline (poor for Hebrew) | Free |
| Amazon Transcribe | Commercial | Reportedly higher accuracy than ivrit-ai | Pay-per-use |
| ElevenLabs Scribe | Commercial | Mixed -- best-in-class on some benchmarks, drops text on long-form | Paid |
| SpeechText.AI | Commercial | Claims 92-95% accuracy on IVRIT benchmarks | Paid |
| Google Cloud STT | Commercial | Supports Hebrew, no direct comparison | Pay-per-use |
| Deepgram Nova | Commercial | Hebrew support requested; ivrit-ai offered dataset | Paid |

ivrit-ai is the only significant open-source provider. No competing open-source Hebrew ASR project exists.

## Pros and Cons

| Pros | Cons |
|------|------|
| World's largest Hebrew audio corpus (22,000+ hours) | Small volunteer team (4-5 people, no compensation) |
| 29% WER improvement over baseline Whisper (peer-reviewed) | ICLR 2024 rejection for foundational paper |
| Free for commercial use (custom permissive license) | Accuracy still below commercial alternatives |
| Multiple model formats (HF, CT2, GGML, ONNX) | Language detection/translation degraded by design |
| Active development through Feb 2026 | Zero Twitter/X and limited English-language visibility |
| Hebrew Transcription Leaderboard establishes standards | Sustainability risk -- donations fund only compute |
| Interspeech 2025 publication validates approach | Long-form transcription (>30s) historically problematic |
| Cost-efficient training (~60h on 8x A40) | Three repos missing explicit licenses |
| Expanding to Yiddish (yi-whisper models) | No code-switching support (Hebrew-English mixing) |
| Real-world adoption (Adobe workaround, WhatsApp bot, Vibe) | Limited independent benchmarking (one Medium article) |

## Sources

### GitHub
- https://github.com/ivrit-ai (organization, 10 repos)
- https://github.com/ivrit-ai/ivrit.ai (main repo)
- https://github.com/ivrit-ai/asr-training (training recipes)

### HuggingFace
- https://huggingface.co/ivrit-ai (models and datasets)
- https://huggingface.co/ivrit-ai/whisper-large-v3 (flagship model)
- https://huggingface.co/ivrit-ai/whisper-large-v3-turbo (turbo variant)
- https://huggingface.co/spaces/ivrit-ai/hebrew-transcription-leaderboard

### Academic
- https://arxiv.org/abs/2307.08720 (dataset paper)
- https://www.isca-archive.org/interspeech_2025/marmor25_interspeech.pdf (Interspeech 2025)
- https://openreview.net/forum?id=aOPTDchLBz (ICLR 2024 submission)

### Blog Posts
- https://www.ivrit.ai/en/2025/02/13/training-whisper/ (Whisper Turbo training)
- https://medium.com/@DormanDaniel/comparing-whisper-whisper-ft-and-amazon-transcribe-for-hebrew-e297846bdd24 (independent benchmark)
- https://medium.com/@yoad/having-fun-scraping-hebrew-transcription-training-data-5a78a0eb22c5 (data scraping)

### LinkedIn
- https://www.linkedin.com/company/ivrit-ai
- https://www.linkedin.com/in/yairlifshitz/
- https://www.linkedin.com/in/yanir-marmor/
- https://www.linkedin.com/posts/yairlifshitz_i-write-most-posts-about-ivritai-in-hebrew-activity-7301856843180466179-uH1s (ElevenLabs evaluation)

### Community
- https://github.com/thewh1teagle/vibe/discussions/27 (Vibe app integration)
- https://github.com/orgs/deepgram/discussions/903 (Deepgram Hebrew request)
- https://github.com/ShmuelRonen/hebrew_whisper (third-party Hebrew Whisper tool)
- https://www.patreon.com/ivrit_ai (donations)

---
*Generated by Research Agent on 2026-02-19*
